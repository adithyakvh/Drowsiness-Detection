{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Drowsiness Detection",
      "provenance": [],
      "collapsed_sections": [
        "vKjbaPwcs6bf",
        "f4aA5cIvG3ep"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning for Drowsiness Detection"
      ],
      "metadata": {
        "id": "GYPxl3A_1cSb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, download the data from this link:\n",
        "http://vlm1.uta.edu/~athitsos/projects/drowsiness/ \\\\\n",
        "Note that there is NOT enough space in standard colab to fit the whole dataset.\n",
        "As such, we recommend doing feature extraction, face alignment, and feature engineering on a local machine and uploading the processed data back to google drive for training"
      ],
      "metadata": {
        "id": "hQKaRKbd1CKh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "LEMRJDAb0_wG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Face Extraction and Alignment"
      ],
      "metadata": {
        "id": "vKjbaPwcs6bf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"face_detection.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/github/dortmans/ml_notebooks/blob/master/face_detection.ipynb\n",
        "\n",
        "# Face detection using pre-trained model\n",
        "\n",
        "We use following blog as a reference:\n",
        "[Face detection with OpenCV and deep learning](https://www.pyimagesearch.com/2018/02/26/face-detection-with-opencv-and-deep-learning/)\n",
        "\n",
        "Import required Python libraries\n",
        "\"\"\"\n",
        "\n",
        "import imutils\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import logging\n",
        "\n",
        "def detectFace(image, model):\n",
        "\n",
        "  # resize it to have a maximum width of 400 pixels\n",
        "  im_resize = imutils.resize(image, width=400)\n",
        "\n",
        "  # Use the [dnn.blobFromImage](https://www.pyimagesearch.com/2017/11/06/deep-learning-opencvs-blobfromimage-works/) function to construct an input blob by resizing the image to a fixed 300x300 pixels and then normalizing it.\n",
        "  blob = cv2.dnn.blobFromImage(cv2.resize(im_resize, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
        "\n",
        "  #  Pass the blob through the neural network and obtain the detections and predictions\n",
        "  model.setInput(blob)\n",
        "  detections = model.forward()\n",
        "\n",
        "  faces_bbox = []\n",
        "  # Loop over the detections and draw boxes around the detected faces\n",
        "  for i in range(0, detections.shape[2]):\n",
        "\n",
        "    # extract the confidence (i.e., probability) associated with the prediction\n",
        "    confidence = detections[0, 0, i, 2]\n",
        "\n",
        "    # filter out weak detections by ensuring the `confidence` is\n",
        "    # greater than the minimum confidence threshold\n",
        "    if confidence > 0.5:\n",
        "      # compute the (x, y)-coordinates of the bounding box for the object\n",
        "      (h, w) = im_resize.shape[:2]\n",
        "      box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "      (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\n",
        "      faces_bbox.append([startX, startY, endX, endY])\n",
        "\n",
        "  return faces_bbox, im_resize\n",
        "\n",
        "def detectAndSaveFace(image, model, image_dir, image_name, transpose=False, flip=False):\n",
        "\n",
        "  if(transpose):\n",
        "    image = image.transpose(1,0,2)\n",
        "  \n",
        "  if(flip):\n",
        "    image = cv2.flip(image, -1)\n",
        "\n",
        "  faces, img = detectFace(image, model);\n",
        "\n",
        "  save_dir = './FacesDataSet/' + image_dir\n",
        "  \n",
        "  save_name = os.path.splitext(image_name)[0] + '_detected_face_'\n",
        "  extension = os.path.splitext(image_name)[1]\n",
        "\n",
        "  if(os.path.isdir(save_dir) == False):\n",
        "    os.mkdir(save_dir)\n",
        "\n",
        "  for i,face in enumerate(faces):\n",
        "    face_im = cv2.resize(img[face[1]:face[-1], face[0]:face[2],:], (224,224))\n",
        "    cv2.imwrite(save_dir + '/' + save_name + str(i) + extension, face_im)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "  prototxt = 'deploy.prototxt'\n",
        "  model = 'res10_300x300_ssd_iter_140000.caffemodel'\n",
        "  model = cv2.dnn.readNetFromCaffe(prototxt, model)\n",
        "\n",
        "  tp_images = {\n",
        "    \"01\":['0'],\n",
        "    \"02\":['5','10'],\n",
        "    \"03\":['0'],\n",
        "    \"05\":['0','5','10'],\n",
        "    \"07\":['0','5'],\n",
        "    \"08\":['0','5','10'],\n",
        "    \"10\":['0','5','10'],\n",
        "    \"13\":['0','5','10'],\n",
        "    \"14\":['5'],\n",
        "    \"19\":['0','5','10'],\n",
        "    \"20\":['0'],\n",
        "    \"21\":['0','5','10'],\n",
        "    \"22\":['0','5','10'],\n",
        "    \"25\":['0','5','10'],\n",
        "    \"26\":['5'],\n",
        "    \"28\":['0','5','10'],\n",
        "    \"30\":['10'],\n",
        "    \"33\":['0','5','10'],\n",
        "    \"38\":['0','5','10'],\n",
        "    \"41\":['5'],\n",
        "    \"42\":['0','5','10'],\n",
        "    \"43\":['0','5','10'],\n",
        "    \"47\":['0','5','10'],\n",
        "    \"49\":['0','5','10'],\n",
        "    \"50\":['0','5','10'],\n",
        "    \"52\":['0','5','10'],\n",
        "    \"53\":['0','5','10'],\n",
        "    \"56\":['0','5','10'],\n",
        "    \"57\":['0','5','10'],\n",
        "    \"58\":['0','5','10'],\n",
        "    \"59\":['0','5','10']\n",
        "  }\n",
        "\n",
        "  flip_images = {\n",
        "    \"07\":['0','5'],\n",
        "    \"08\":['0','5','10'],\n",
        "    \"13\":['0','5','10'],\n",
        "    \"14\":['5'],\n",
        "    \"20\":['0'],\n",
        "    \"25\":['0','5','10'],\n",
        "    \"26\":['5','10'],\n",
        "    \"30\":['10'],\n",
        "    \"33\":['0','5','10'],\n",
        "    \"38\":['0','5','10'],\n",
        "    \"42\":['0','5','10'],\n",
        "    \"45\":['0','5','10'],\n",
        "    \"47\":['0','5','10'],\n",
        "    \"49\":['0','5','10'],\n",
        "    \"58\":['0','5','10']\n",
        "  }\n",
        "  \n",
        "  list_of_folders = []\n",
        "  for i in range(2,3):\n",
        "    dir = './DataSet/' + 'Sampled_frames' + str(i+1)\n",
        "    sub_dir = 'Sampled_faces' + str(i+1)\n",
        "    for j in range(2):\n",
        "      folder_dir = dir + '_part' + str(j+1)\n",
        "      sub_dir += '_part' + str(j+1)\n",
        "      \n",
        "      if(os.path.isdir('./FacesDataSet/' + sub_dir) == False):\n",
        "        os.mkdir('./FacesDataSet/' + sub_dir)\n",
        "\n",
        "      # print(folder_dir)\n",
        "      for k in range(2,3):     \n",
        "        sample_num = (i)*12 + j*6 + k\n",
        "        print(\"Working on \" + str(sample_num) + \"...\")\n",
        "        input()\n",
        "        sample_dir = \"{:02d}\".format(sample_num)\n",
        "        list_of_folders.append(folder_dir + \"/\" + sample_dir)\n",
        "        person_sub_dir = sub_dir +  '/' + sample_dir\n",
        "\n",
        "        if(os.path.isdir('./FacesDataSet/' + person_sub_dir) == False):\n",
        "          os.mkdir('./FacesDataSet/' + person_sub_dir)\n",
        "        \n",
        "        tp_status = None\n",
        "        if sample_dir in tp_images.keys():\n",
        "          tp_status = tp_images[sample_dir]\n",
        "\n",
        "        flip_status = None\n",
        "        if sample_dir in flip_images.keys():\n",
        "          flip_status = flip_images[sample_dir]\n",
        "        \n",
        "        for label in [10]:#[0, 5, 10]:\n",
        "          images_path = folder_dir + '/' + sample_dir + '/' + str(label)\n",
        "          image_sub_dir = person_sub_dir +  '/' + str(label)\n",
        "          try:\n",
        "            for image_name in os.listdir(images_path):\n",
        "              image_dir = images_path + '/' + image_name\n",
        "              img = cv2.imread(image_dir)\n",
        "              if img is not None:\n",
        "                \n",
        "                transpose = False\n",
        "                if(tp_status is not None):\n",
        "                  if(str(label) in tp_status):\n",
        "                    transpose = True\n",
        "\n",
        "                flip = False\n",
        "                if(flip_status is not None):\n",
        "                  if(str(label) in flip_status):\n",
        "                    flip = True\n",
        "                \n",
        "                detectAndSaveFace(img, model, image_sub_dir, image_name, transpose, flip)\n",
        "\n",
        "          except:\n",
        "            logging.error(images_path + \" does not exist!\")\n",
        "        print(\"Completed \" + str(sample_num))      \n",
        "  # for eachImage in\n",
        "  print(\"LEN\", len(list_of_folders))"
      ],
      "metadata": {
        "id": "4sjX9YYHs93O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optional: Downsample the Dataset (less number of frames)"
      ],
      "metadata": {
        "id": "04t0ydlxymYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_image_paths = []\n",
        "for classes_path in glob.glob(dataset_path + '/*'):\n",
        "    classes.append(classes_path.split('/')[-1].split('\\\\')[-1])\n",
        "    for train_idx in range(1,49):\n",
        "        if train_idx < 10:\n",
        "            train_image_paths.append(glob.glob(classes_path + '/0' + str(train_idx) + ' *'))\n",
        "        else:\n",
        "            train_image_paths.append(glob.glob(classes_path + '/' + str(train_idx) + ' *'))\n",
        "        \n",
        "print(\"Before Flattening :\",len(train_image_paths))\n",
        "train_image_paths = list(flatten(train_image_paths))\n",
        "print(\"After Flattening :\",len(train_image_paths))"
      ],
      "metadata": {
        "id": "0ivxfN3nyfTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_image_paths = []\n",
        "for classes_path in glob.glob(dataset_path + '/*'):\n",
        "    classes.append(classes_path.split('/')[-1].split('\\\\')[-1])\n",
        "    for train_idx in range(49,61):\n",
        "        if train_idx < 10:\n",
        "            valid_image_paths.append(glob.glob(classes_path + '/0' + str(train_idx) + ' *'))\n",
        "        else:\n",
        "            valid_image_paths.append(glob.glob(classes_path + '/' + str(train_idx) + ' *'))\n",
        "    \n",
        "print(\"Before Flattening :\"len(valid_image_paths))\n",
        "valid_image_paths = list(flatten(valid_image_paths))\n",
        "print(\"After Flattening :\",len(valid_image_paths))"
      ],
      "metadata": {
        "id": "H07KkXMny3N7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import time\n",
        "destination_train = \"./Data/downsampled_clean/val/\"\n",
        "for each_img in valid_image_paths:\n",
        "    file_number = \"/\".join(each_img.split(\"\\\\\")).split('(')[-1].split(')')[0]\n",
        "    if int(file_number) % 10 == 0:\n",
        "        shutil.copy(\"/\".join(each_img.split(\"\\\\\")), destination_train + \"/\".join(each_img.split(\"\\\\\")).split(\"/\")[-2])"
      ],
      "metadata": {
        "id": "OAQ-IktYy2PF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "destination_train = \"./Data/downsampled_clean/train/\"\n",
        "for each_img in train_image_paths:\n",
        "    file_number = \"/\".join(each_img.split(\"\\\\\")).split('(')[-1].split(')')[0]\n",
        "    if int(file_number) % 10 == 0:\n",
        "        shutil.copy(\"/\".join(each_img.split(\"\\\\\")), destination_train + \"/\".join(each_img.split(\"\\\\\")).split(\"/\")[-2])"
      ],
      "metadata": {
        "id": "0iSAmIAAy2YX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting the Data into Training and Testing"
      ],
      "metadata": {
        "id": "q8fNQEb5yJ2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################\n",
        "#       Create Train, Valid and Test sets\n",
        "####################################################\n",
        "dataset_path = 'FacesDataSet/Clean_Split_Data/' \n",
        "\n",
        "train_to_test_ratio = 0.8\n",
        "train_to_validation_ratio = 0.8\n",
        "\n",
        "image_paths = [] #to store image paths in list\n",
        "classes = [] #to store class values\n",
        "\n",
        "for classes_path in glob.glob(dataset_path + '/*'):\n",
        "    classes.append(classes_path.split('/')[-1].split('\\\\')[-1])\n",
        "    image_paths.append(glob.glob(classes_path + '/*'))\n",
        "    \n",
        "image_paths = list(flatten(image_paths))\n",
        "# random.shuffle(image_paths)\n",
        "\n",
        "\n",
        "print('train_image_path example: ', image_paths[-1])\n",
        "print('class example: ', classes[-1])\n",
        "\n",
        "\n",
        "#split train valid from train paths (80,20)\n",
        "train_image_paths, test_image_paths = image_paths[:int(train_to_test_ratio*len(image_paths))], image_paths[int(train_to_test_ratio*len(image_paths)):]\n",
        "# train_image_paths, valid_image_paths = train_image_paths[:int(train_to_validation_ratio*len(train_image_paths))], train_image_paths[int(train_to_validation_ratio*len(train_image_paths)):]\n",
        "\n",
        "print(\"\\nTrain size: {}\\nValid size: {}\\n\".format(len(train_image_paths), len(test_image_paths)))\n",
        "\n",
        "dataset_sizes = {'train': len(train_image_paths),\n",
        "                'val' : len(test_image_paths)}"
      ],
      "metadata": {
        "id": "V35jRLlIyMij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Engineering"
      ],
      "metadata": {
        "id": "mf12tlQ1roMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eye_aspect_ratio(eye):\n",
        "\tA = distance.euclidean(eye[1], eye[5])\n",
        "\tB = distance.euclidean(eye[2], eye[4])\n",
        "\tC = distance.euclidean(eye[0], eye[3])\n",
        "\tear = (A + B) / (2.0 * C)\n",
        "\treturn ear\n",
        "def mouth_aspect_ratio(mouth):\n",
        "    A = distance.euclidean(mouth[14], mouth[18])\n",
        "    C = distance.euclidean(mouth[12], mouth[16])\n",
        "    mar = (A ) / (C)\n",
        "    return mar\n",
        "def circularity(eye):\n",
        "    A = distance.euclidean(eye[1], eye[4])\n",
        "    radius  = A/2.0\n",
        "    Area = math.pi * (radius ** 2)\n",
        "    p = 0\n",
        "    p += distance.euclidean(eye[0], eye[1])\n",
        "    p += distance.euclidean(eye[1], eye[2])\n",
        "    p += distance.euclidean(eye[2], eye[3])\n",
        "    p += distance.euclidean(eye[3], eye[4])\n",
        "    p += distance.euclidean(eye[4], eye[5])\n",
        "    p += distance.euclidean(eye[5], eye[0])\n",
        "    return 4 * math.pi * Area /(p**2)\n",
        "def mouth_over_eye(eye):\n",
        "    ear = eye_aspect_ratio(eye)\n",
        "    mar = mouth_aspect_ratio(eye)\n",
        "    mouth_eye = mar/ear\n",
        "    return mouth_eye\n",
        "def getFrame(sec):\n",
        "    start = 180000\n",
        "    vidcap.set(cv2.CAP_PROP_POS_MSEC, start + sec*1000)\n",
        "    hasFrames,image = vidcap.read()\n",
        "    return hasFrames, image"
      ],
      "metadata": {
        "id": "4TBfzstyrqpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os.path\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "from mlxtend.image import extract_face_landmarks\n",
        "import os\n",
        "from scipy.spatial import distance\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "data = []\n",
        "labels = []\n",
        "failed_images=[]\n",
        "break_flag = False\n",
        "pre_path = '/content/drive/MyDrive/CIS 520 Project/Data_file_for_cis520_project'\n",
        "working_path = '/content/drive/MyDrive/CIS 520 Project/Data_file_for_cis520_project/FacesDataSet/Sampled_faces1_part1'\n",
        "save_path = pre_path + '/Feature Extraction'\n",
        "#i = 0\n",
        "count_fail=0\n",
        "count_success=0\n",
        "  # i += 1\n",
        "# print(\"Each folder\", eachFolder)\n",
        "for i in os.listdir(working_path):\n",
        "  #new directory location\n",
        "  path = save_path + \"/\"+ str(i)\n",
        "  os.mkdir(path)\n",
        "\n",
        "  for num in [0, 5, 10]:\n",
        "    data = []\n",
        "    labels = []\n",
        "    features = []\n",
        "    count_fail=0\n",
        "    count_success=0\n",
        "\n",
        "    full_dir = working_path + \"/\" + str(i) + \"/\" + str(num)\n",
        "    for image in os.listdir(full_dir):\n",
        "      img = Image.open(full_dir + \"/\" + image)\n",
        "      img = np.asarray(img)\n",
        "      img_T = img.transpose((1, 0, 2))\n",
        "\n",
        "      landmarks = extract_face_landmarks(img)\n",
        "      landmarks_T = extract_face_landmarks(img_T)\n",
        "\n",
        "      if sum(sum(landmarks)) != 0:\n",
        "        data.append(landmarks)\n",
        "        labels.append(num)\n",
        "        count_success+=1\n",
        "      elif sum(sum(landmarks_T)) != 0:\n",
        "        data.append(landmarks_T)\n",
        "        labels.append(num)\n",
        "        count_success+=1\n",
        "      else:\n",
        "        count_fail+=1\n",
        "        failed_images.append([image,i,num])\n",
        "\n",
        "    for d in data:\n",
        "      eye = d[36:68]\n",
        "      ear = eye_aspect_ratio(eye)\n",
        "      mar = mouth_aspect_ratio(eye)\n",
        "      cir = circularity(eye)\n",
        "      mouth_eye = mouth_over_eye(eye)\n",
        "      features.append([ear, mar, cir, mouth_eye])\n",
        "    features = np.array(features)\n",
        "    labels = np.array(labels)\n",
        "    print(\"Faces not detected =\",count_fail)        \n",
        "    \n",
        "    np.savetxt(path + '/' + str(num) + \"_features\" + \".csv\", features, delimiter = \",\")\n",
        "    np.savetxt(path + '/' + str(num) +\"_labels\" + \".csv\", labels, delimiter = \",\")        "
      ],
      "metadata": {
        "id": "5Wa6j8rur8Dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Imputation - Mean Data Imputation"
      ],
      "metadata": {
        "id": "XqHPrfSXvfhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import numpy.ma as ma\n",
        "from numpy import linalg as LA\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial import distance\n",
        "import random\n",
        "import pandas\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "Y6QPKDy3vP9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def meanImpute(X_miss):\n",
        "  '''\n",
        "  Returns :\n",
        "    X_imputed which has mean b the corresponding column instead of the missing values and same shape as X_miss.\n",
        "  '''\n",
        "  X_imputed = X_miss.copy()\n",
        "  \n",
        "  not_nan = ~np.isnan(X_imputed)\n",
        "  column_mean = np.nanmean(X_imputed, axis=0)\n",
        "  inds = np.where(np.isnan(X_imputed)) \n",
        "  X_imputed[inds] = np.take(column_mean, inds[1])\n",
        " \n",
        "  assert X_imputed.shape == X_miss.shape\n",
        "  # np.savetxt(\"/content/drive/MyDrive/CIS 520 Project/NAN TRIAL.txt\", X_imputed)\n",
        "  return X_imputed"
      ],
      "metadata": {
        "id": "-y80B6s0vjAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "path = \"/content/drive/MyDrive/CIS 520 Project/Data_file_for_cis520_project/Landmarks with NANs Extract/Landmarks with NANs\"\n",
        "for eachPerson in os.listdir(path):\n",
        "  for eachState in [0, 5, 10]:\n",
        "    filePath = path + \"/\" + eachPerson + \"/\" + str(eachState) + \"_features.csv\"\n",
        "    y_filePath = path + \"/\" + eachPerson + \"/\" + str(eachState) + \"_labels.csv\"\n",
        "\n",
        "    data_csv_X = pandas.read_csv(filePath)\n",
        "    data_csv_y = pandas.read_csv(y_filePath)\n",
        "    data_csv_X = data_csv_X.to_numpy()\n",
        "    data_csv_y = data_csv_y.to_numpy()\n",
        "    X_imputed = meanImpute(data_csv_X)\n",
        "    \n",
        "    savePath = \"/content/drive/MyDrive/CIS 520 Project/Data_file_for_cis520_project/Landmarks without NANS/\" + \\\n",
        "                str(eachState) + \\\n",
        "                \"_features.csv\" \n",
        "    print(savePath)\n",
        "    np.savetxt(savePath, X_imputed.astype(int))"
      ],
      "metadata": {
        "id": "ZH0sDL92vtnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Drowsiness MNIST (DNIST) Creation"
      ],
      "metadata": {
        "id": "DswycqQWI2SQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas.core.common import flatten\n",
        "from sklearn.model_selection import train_test_split\n",
        "import copy\n",
        "import numpy as np\n",
        "import random\n",
        "import time, os\n",
        "import glob\n",
        "\n",
        "def draw_landmarks(img, landmark_tuple):\n",
        "    routes = []\n",
        " \n",
        "    for i in range(5):\n",
        "        from_coordinate = landmark_tuple[i]\n",
        "        to_coordinate = landmark_tuple[i+1]\n",
        "        img = cv2.line(img, from_coordinate, to_coordinate, 1, 2)\n",
        " \n",
        "    img = cv2.line(img, landmark_tuple[4], landmark_tuple[0], 1, 2)\n",
        "\n",
        "    for i in range(6, 11):\n",
        "        from_coordinate = landmark_tuple[i]\n",
        "        to_coordinate = landmark_tuple[i+1]\n",
        "        img = cv2.line(img, from_coordinate, to_coordinate, 1, 2)\n",
        "\n",
        "    img = cv2.line(img, landmark_tuple[11], landmark_tuple[6], 1, 2)\n",
        "\n",
        "    for i in range(12, 23):\n",
        "        from_coordinate = landmark_tuple[i]\n",
        "        to_coordinate = landmark_tuple[i+1]\n",
        "        img = cv2.line(img, from_coordinate, to_coordinate, 1, 2)\n",
        "        \n",
        "    img = cv2.line(img, landmark_tuple[23], landmark_tuple[12], 1, 2)\n",
        "    \n",
        "    img = cv2.line(img, landmark_tuple[12], landmark_tuple[24], 1, 2)\n",
        "    \n",
        "    for i in range(24, len(landmark_tuple) - 1):\n",
        "        from_coordinate = landmark_tuple[i]\n",
        "        to_coordinate = landmark_tuple[i+1]\n",
        "        img = cv2.line(img, from_coordinate, to_coordinate, 1, 2)\n",
        "        \n",
        "    img = cv2.line(img, landmark_tuple[len(landmark_tuple) - 1], landmark_tuple[24], 1, 2)\n",
        "  \n",
        "X = []\n",
        "y = []\n",
        "\n",
        "save_folder = \"DNIST\"  \n",
        "    \n",
        "\n",
        "for j in range(1, 61): \n",
        "    if (j < 10):\n",
        "        s = \"0\"+str(j)\n",
        "    else:\n",
        "        s = str(j)\n",
        "\n",
        "    for i in [0, 5, 10]:\n",
        "        loaded_X = np.loadtxt(\"Adi_Features/Feature Extraction/\"+s+\"/\"+str(i)+\"_features.csv\", delimiter=',')\n",
        "        if(len(loaded_X) == 0):\n",
        "            continue\n",
        "\n",
        "        important_x = np.array(loaded_X)[:,72:]\n",
        "        x_locs = np.int64(important_x[:,::2])\n",
        "        y_locs = np.int64(important_x[:,1::2])\n",
        "        images = np.zeros((x_locs.shape[0],224,224))\n",
        "        for k, (x,y) in enumerate(zip(x_locs, y_locs)):\n",
        "            images[k][y,x] = 1\n",
        "            draw_landmarks(images[k], list(zip(x,y)))\n",
        "\n",
        "            if j < 49:\n",
        "                save_dir = save_folder + \"/train/\"+str(i) + \"/\"+ s + \"_\" + str(k) + \".jpeg\"\n",
        "            else:\n",
        "                save_dir = save_folder + \"/val/\"+str(i) + \"/\"+ s + \"_\" + str(k) + \".jpeg\"\n",
        "            cv2.imwrite(save_dir, images[k]*255)"
      ],
      "metadata": {
        "id": "dx8erQH3I5q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Time Series Data"
      ],
      "metadata": {
        "id": "f4aA5cIvG3ep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = []\n",
        "y = []\n",
        "\n",
        "for j in range(1, 61): \n",
        "    if (j < 10):\n",
        "        s = \"0\"+str(j)\n",
        "    else:\n",
        "        s = str(j)\n",
        "    for i in [0, 5, 10]:\n",
        "        loaded_X = np.loadtxt(\"/content/drive/MyDrive/Data_file_for_cis520_project/All_Landmarks/\"+s+\"/\"+str(i)+\"_features.csv\", delimiter=',')\n",
        "        idx = np.arange(30, loaded_X.shape[0], 30)\n",
        "        time_series = np.split(loaded_X, idx)\n",
        "        if len(time_series)>=2:\n",
        "            del time_series[-1]\n",
        "            del time_series[0]\n",
        "            X += time_series\n",
        "            y_series = [int(i/5)] * len(time_series)\n",
        "            y += y_series\n",
        "\n",
        "        idx = np.arange(5, loaded_X.shape[0], 30)\n",
        "        time_series = np.split(loaded_X, idx)\n",
        "        if len(time_series)>=2:\n",
        "            del time_series[-1]\n",
        "            del time_series[0]\n",
        "            X += time_series\n",
        "            y_series = [int(i/5)] * len(time_series)\n",
        "            y += y_series\n",
        "\n",
        "        idx = np.arange(10, loaded_X.shape[0], 30)\n",
        "        time_series = np.split(loaded_X, idx)\n",
        "        if len(time_series)>=2:\n",
        "            del time_series[-1]\n",
        "            del time_series[0]\n",
        "            X += time_series\n",
        "            y_series = [int(i/5)] * len(time_series)\n",
        "            y += y_series\n",
        "\n",
        "        idx = np.arange(15, loaded_X.shape[0], 30)\n",
        "        time_series = np.split(loaded_X, idx)\n",
        "        if len(time_series)>=2:\n",
        "            del time_series[-1]\n",
        "            del time_series[0]\n",
        "            X += time_series\n",
        "            y_series = [int(i/5)] * len(time_series)\n",
        "            y += y_series\n",
        "\n",
        "        idx = np.arange(20, loaded_X.shape[0], 30)\n",
        "        time_series = np.split(loaded_X, idx)\n",
        "        if len(time_series)>=2:\n",
        "            del time_series[-1]\n",
        "            del time_series[0]\n",
        "            X += time_series\n",
        "            y_series = [int(i/5)] * len(time_series)\n",
        "            y += y_series\n",
        "\n",
        "        idx = np.arange(25, loaded_X.shape[0], 30)\n",
        "        time_series = np.split(loaded_X, idx)\n",
        "        if len(time_series)>=2:\n",
        "            del time_series[-1]\n",
        "            del time_series[0]\n",
        "            X += time_series\n",
        "            y_series = [int(i/5)] * len(time_series)\n",
        "            y += y_series"
      ],
      "metadata": {
        "id": "Nj9IEbX6G2XL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X))\n",
        "print(X[0].shape)\n",
        "print(np.array(X).shape)\n",
        "print(len(y))"
      ],
      "metadata": {
        "id": "CeiYaPUKG_fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(np.array(X)[:,:,74:120], np.array(y), test_size=0.2, shuffle=False)\n",
        "# X_train, X_test, y_train, y_test = train_test_split(np.array(X)[:,:,:], np.array(y), test_size=0.2, shuffle=False)\n",
        "\n",
        "from sklearn import preprocessing\n",
        "for i, x_item in enumerate(X_train):\n",
        "    scaler = preprocessing.StandardScaler().fit(x_item[:,::2])\n",
        "    X_train[i][:,::2] = scaler.transform(x_item[:,::2])\n",
        "\n",
        "    scaler = preprocessing.StandardScaler().fit(x_item[:,1::2])\n",
        "    X_train[i][:,1::2] = scaler.transform(x_item[:,1::2])\n",
        "\n",
        "for i, x_item in enumerate(X_test):\n",
        "    scaler = preprocessing.StandardScaler().fit(x_item[:,::2])\n",
        "    X_test[i][:,::2] = scaler.transform(x_item[:,::2])\n",
        "\n",
        "    scaler = preprocessing.StandardScaler().fit(x_item[:,1::2])\n",
        "    X_test[i][:,1::2] = scaler.transform(x_item[:,1::2])\n",
        "\n",
        "shuffler1 = np.random.permutation(len(X_train))\n",
        "shuffler2 = np.random.permutation(len(X_test))\n",
        "X_train = X_train[shuffler1]\n",
        "y_train = y_train[shuffler1]\n",
        "X_test = X_test[shuffler2]\n",
        "y_test = y_test[shuffler2]"
      ],
      "metadata": {
        "id": "25159eoMHA3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Deep Learning Approaches"
      ],
      "metadata": {
        "id": "G7dxfaIazk0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Transfer Learning"
      ],
      "metadata": {
        "id": "aR00z4shxQU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf \n",
        "\n",
        "if tf.test.gpu_device_name(): \n",
        "    \n",
        "\n",
        "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
        "\n",
        "else:\n",
        "\n",
        "    print(\"Please install GPU version of TF\")"
      ],
      "metadata": {
        "id": "fimBmM1U0dqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "zdnz1tt50h2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.applications.resnet import ResNet50\n",
        "# from tensorflow.keras.applications.resnet import preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "import glob\n",
        "import random\n",
        "import shutil\n",
        "# from glob import glob\n",
        "IMAGE_SIZE = [224, 224]\n",
        "import os"
      ],
      "metadata": {
        "id": "idC6ernE0jSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, You can uncomment your model of choice and use that for transfer learning."
      ],
      "metadata": {
        "id": "o1n7XpQC0LtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Vgg 16 library as shown below and add preprocessing layer to the front of VGG\n",
        "# Here we will be using imagenet weights\n",
        "\n",
        "vgg16 = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n",
        "# res50 = ResNet50(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n",
        "# res18 = ResNet50(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)"
      ],
      "metadata": {
        "id": "mEPJLe0J0kor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also chose between freezing the pretrained model weights for faster training, or alternatively training the whole model for potentially better accuracy."
      ],
      "metadata": {
        "id": "r7LZoGCs0UwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# don't train existing weights\n",
        "# for layer in vgg16.layers:\n",
        "#     layer.trainable = False\n",
        "\n",
        "# don't train existing weights\n",
        "# for layer in res50.layers:\n",
        "#     layer.trainable = False\n",
        "\n",
        "# don't train existing weights\n",
        "# for layer in res18.layers:\n",
        "#     layer.trainable = False"
      ],
      "metadata": {
        "id": "HaEAnFCc0rj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the Image Data Generator to import the images from the dataset\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "test_datagen = ImageDataGenerator()"
      ],
      "metadata": {
        "id": "5zDgoRbj0tew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, you can uncomment your dataset of choice:\n",
        "\n",
        "\n",
        "1.   Downsampled dataset with fewer frames.\n",
        "2.   Original Dataset with ~ 600 frames per video\n",
        "3.   DNIST - Drowsiness MNIST Dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "2ee2wuhn1Ejd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure you provide the same target size as initialied for the image size\n",
        "training_set = train_datagen.flow_from_directory(\"Data/downsampled_clean/train\",\n",
        "                                                #  \"FacesDataSet/clean_test_train_split/train\",\n",
        "                                                #  \"DNIST/train\",\n",
        "                                                 target_size = (224, 224),\n",
        "                                                 batch_size = 64,\n",
        "                                                 class_mode = 'categorical', \n",
        "                                                 shuffle = True)"
      ],
      "metadata": {
        "id": "tSk7DTLS0uba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = test_datagen.flow_from_directory(\"Data/downsampled_clean/val\",\n",
        "                                            #  \"FacesDataSet/clean_test_train_split/val\",\n",
        "                                            #  \"DNIST/val\",\n",
        "                                            target_size = (224, 224),\n",
        "                                            batch_size = 64,\n",
        "                                            class_mode = 'categorical', \n",
        "                                            shuffle = True)"
      ],
      "metadata": {
        "id": "P3-H_g0P0vSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, uncomment your model of choice"
      ],
      "metadata": {
        "id": "F1cMzwYX0z7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "feature_extractor_layer = hub.KerasLayer(\n",
        "    # res18,\n",
        "    # res50,\n",
        "    vgg16,\n",
        "    input_shape=(224, 224, 3),\n",
        "    trainable=False)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  feature_extractor_layer,\n",
        "  tf.keras.layers.Dense(3)\n",
        "])\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  tf.keras.layers.RandomFlip('horizontal'),\n",
        "  tf.keras.layers.RandomRotation(0.2),\n",
        "])"
      ],
      "metadata": {
        "id": "nWulfLAS0wZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_batch, label_batch = next(iter(training_set))\n",
        "feature_batch = model(image_batch)\n",
        "print(feature_batch.shape)"
      ],
      "metadata": {
        "id": "c7q6EBin0xJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global_average_layer = tf.keras.layers.GlobalMaxPool2D()\n",
        "feature_batch_average = global_average_layer(feature_batch)\n",
        "print(feature_batch_average.shape)"
      ],
      "metadata": {
        "id": "QHs3csqp0yIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_layer = tf.keras.layers.Dense(3)\n",
        "prediction_batch = prediction_layer(feature_batch_average)\n",
        "print(prediction_batch.shape)"
      ],
      "metadata": {
        "id": "JywC3z160y2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
        "x = data_augmentation(inputs)\n",
        "x = preprocess_input(x)\n",
        "# x = model(x, training=False)\n",
        "x = model(x, training=True)\n",
        "x = global_average_layer(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "outputs = prediction_layer(x)\n",
        "model = tf.keras.Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "mDqWCFzj0zqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_learning_rate = 0.0001\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=base_learning_rate,momentum=0.9),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "UEXH2HrA00_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "1nEWB5ar018G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initial evalutaion of the model before training"
      ],
      "metadata": {
        "id": "GI8Yxcgq04Qu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initial_epochs = 50\n",
        "\n",
        "loss0, accuracy0 = model.evaluate(test_set)"
      ],
      "metadata": {
        "id": "Hv-xZy5M03Im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit the model and obtain results"
      ],
      "metadata": {
        "id": "c8a4GB_n06r_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(training_set,\n",
        "                    epochs=initial_epochs,\n",
        "                    validation_data=test_set)"
      ],
      "metadata": {
        "id": "gsBPm4dA04o0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM Model"
      ],
      "metadata": {
        "id": "FFFlq2piHFYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_input = len(X_train[0][0])\n",
        "n_hidden = 17\n",
        "n_classes = 3\n",
        "decaying_learning_rate = True\n",
        "learning_rate = 0.0025\n",
        "init_learning_rate = 0.05\n",
        "decay_rate = 0.96\n",
        "decay_steps = 100000\n",
        "global_step = tf.Variable(0, trainable=False)\n",
        "lambd = 0.0015\n",
        "epochs = 500\n",
        "batch_size = 128\n",
        "drop_out = 0.6"
      ],
      "metadata": {
        "id": "RDJZozmsHGRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def LSTM_RNN(_X, _weights, _biases):\n",
        "    _X = tf.transpose(_X, [1, 0, 2])\n",
        "    _X = tf.reshape(_X, [-1, n_input]) \n",
        "    _X = tf.nn.relu(tf.matmul(_X, _weights['hidden']) + _biases['hidden'])\n",
        "    _X = tf.split(_X, n_steps, 0)\n",
        "    lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
        "    lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
        "\n",
        "    lstm_dropout1 = tf.contrib.rnn.DropoutWrapper(lstm_cell_1,input_keep_prob=0.6, output_keep_prob=0.6)\n",
        "    lstm_dropout2 = tf.contrib.rnn.DropoutWrapper(lstm_cell_2,input_keep_prob=0.6, output_keep_prob=0.6)\n",
        "\n",
        "    lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_dropout1, lstm_dropout2], state_is_tuple=True)\n",
        "    outputs, states = tf.contrib.rnn.static_rnn(lstm_cells, _X, dtype=tf.float32)\n",
        "    lstm_last_output = outputs[-1]\n",
        "    return tf.matmul(lstm_last_output, _weights['out']) + _biases['out']\n",
        "\n",
        "def extract_batch_size(_train, _labels, _unsampled, batch_size):\n",
        "    shape = list(_train.shape)\n",
        "    shape[0] = batch_size\n",
        "    batch_s = np.empty(shape)\n",
        "    batch_labels = np.empty((batch_size,1))\n",
        "    \n",
        "    for i in range(batch_size):\n",
        "        uns = list(_unsampled)\n",
        "        index = random.choice(uns)\n",
        "        batch_s[i] = _train[index] \n",
        "        batch_labels[i] = _labels[index]\n",
        "        uns.remove(index)\n",
        "        \n",
        "    return batch_s, batch_labels, _unsampled\n",
        "\n",
        "def one_hot(y_):\n",
        "    y_ = y_.reshape(len(y_))\n",
        "    n_values = int(np.max(y_)) + 1\n",
        "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]"
      ],
      "metadata": {
        "id": "6UN5fMpJHH2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LABELS = [\n",
        "    'alert',\n",
        "    'low vigilant',\n",
        "    'drowsy',\n",
        "]\n",
        "\n",
        "n_steps = 30"
      ],
      "metadata": {
        "id": "Ol2UbvyWHH4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.placeholder(tf.float32, [None, n_steps, n_input])\n",
        "y = tf.placeholder(tf.float32, [None, n_classes])\n",
        "\n",
        "weights = {\n",
        "    'hidden': tf.Variable(tf.random_normal([n_input, n_hidden])), \n",
        "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes], mean=1.0))\n",
        "}\n",
        "biases = {\n",
        "    'hidden': tf.Variable(tf.random_normal([n_hidden])),\n",
        "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
        "}\n",
        "\n",
        "pred = LSTM_RNN(x, weights, biases)\n",
        "\n",
        "l2 = lambd * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred)) + l2\n",
        "if decaying_learning_rate:\n",
        "    learning_rate = tf.train.exponential_decay(init_learning_rate, global_step*batch_size, decay_steps, decay_rate, staircase=True)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost,global_step=global_step)\n",
        "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
      ],
      "metadata": {
        "id": "Fo007BrCHH6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_losses = []\n",
        "test_accuracies = []\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n",
        "saver = tf.train.Saver(max_to_keep=3) \n",
        "init = tf.global_variables_initializer()\n",
        "sess.run(init)\n",
        "iters = epochs * len(X_train)\n",
        "\n",
        "step = 1\n",
        "time_start = time.time()\n",
        "unsampled_indices = range(0,len(X_train))\n",
        "while step * batch_size <= iters:\n",
        "    if len(unsampled_indices) < batch_size:\n",
        "        unsampled_indices = range(0,len(X_train)) \n",
        "    batch_xs, raw_labels, unsampled_indicies = extract_batch_size(X_train, y_train, unsampled_indices, batch_size)\n",
        "    batch_ys = one_hot(raw_labels)\n",
        "    if len(batch_ys[0]) < n_classes:\n",
        "        temp_ys = np.zeros((batch_size, n_classes))\n",
        "        temp_ys[:batch_ys.shape[0],:batch_ys.shape[1]] = batch_ys\n",
        "        batch_ys = temp_ys\n",
        "        \n",
        "    _, loss, acc = sess.run([optimizer, cost, accuracy],feed_dict={x: batch_xs, y: batch_ys})\n",
        "    train_losses.append(loss)\n",
        "    train_accuracies.append(acc)\n",
        "    \n",
        "    if(step % 10 ==0) or (step == 1) or (step * batch_size > iters):\n",
        "        print(\"Step\" + str(step) +  \": learning_rate = \" + \"{:.6f}\".format(sess.run(learning_rate)) + \"\\n\" + \"Training\" + \":   batch_loss = \" + \"{:.6f}\".format(loss) + \", accuracy = \" + \"{:.6f}\".format(acc))\n",
        "#               \":  Learning rate = \" + \"{:.6f}\".format(sess.run(learning_rate)) + \\\n",
        "        loss, acc = sess.run([cost, accuracy],feed_dict={x: X_test,y: one_hot(y_test)})\n",
        "        test_losses.append(loss)\n",
        "        test_accuracies.append(acc)\n",
        "        print(\"Test\" + \":   batch_loss = \" + \"{:.6f}\".format(loss) + \", accuracy = \" + \"{:.6f}\".format(acc))\n",
        "\n",
        "    step += 1\n",
        "    if step % 100 == 0:\n",
        "        saver.save(sess, \"/content/drive/MyDrive/Data_file_for_cis520_project/Siming_LSTM_Model/lstm_asl.ckpt-\" + str(step))\n",
        "         \n",
        "print(\"Optimization Finished!\")\n",
        "\n",
        "writer = tf.summary.FileWriter(\"/content/drive/MyDrive/Data_file_for_cis520_project/Siming_LSTM_Model\",sess.graph)\n",
        "writer.close()\n",
        "\n",
        "one_hot_predictions, accuracy, final_loss = sess.run([pred, accuracy, cost],feed_dict={x:X_test, y: one_hot(y_test)})\n",
        "test_losses.append(final_loss)\n",
        "test_accuracies.append(accuracy)\n",
        "\n",
        "print(\"Final result: \" + \"batch_loss = \" + \"{:.6f}\".format(final_loss) + \", accuracy = \" + \"{:.6f}\".format(accuracy))\n",
        "time_stop = time.time()\n",
        "print(\"Total time:  {}\".format(time_stop - time_start))                                                                                    "
      ],
      "metadata": {
        "id": "J4uKdYzrHH9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k_values = range(1, len(test_accuracies)+1, 1)\n",
        "t_accuracies = [0.3] + [0.3] + train_accuracies[::10]\n",
        "plt.plot(k_values, t_accuracies, color='r', label = 'train accuracies')\n",
        "plt.plot(k_values, test_accuracies, color='g', label='test accuracies')\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"accuracies\")\n",
        "plt.title(\"The accuracies of Model 1\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jJgoCa6AHH_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k_values = range(1, len(test_losses)+1, 1)\n",
        "t_loss = [1.5] + [1.5] + train_losses[::10]\n",
        "plt.plot(k_values, t_loss, color='r', label = 'train loss')\n",
        "plt.plot(k_values, test_losses, color='g', label='test loss')\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"The loss of Model 1\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vq1OXbrvHIBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "font = {\n",
        "    'family' : 'Bitstream Vera Sans',\n",
        "    'weight' : 'bold',\n",
        "    'size'   : 18\n",
        "}\n",
        "matplotlib.rc('font', **font)\n",
        "\n",
        "width = 12\n",
        "height = 12\n",
        "plt.figure(figsize=(width, height))\n",
        "\n",
        "# indep_train_axis = np.array(range(0int(step)))\n",
        "indep_train_axis = np.array(range(batch_size, (len(train_losses)+1)*batch_size, batch_size))\n",
        "#plt.plot(indep_train_axis, np.array(train_losses),     \"g--\", label=\"Train losses\")\n",
        "plt.plot(indep_train_axis, np.array(train_accuracies), \"g--\", label=\"Train accuracies\")\n",
        "\n",
        "# indep_test_axis = np.append(np.array(range(0int(step)))\n",
        "indep_test_axis = np.append(np.array(range(batch_size, len(test_losses)*batch_size*8, batch_size*8)[:-1]), [iters])\n",
        "#plt.plot(indep_test_axis, np.array(test_losses), \"b-\", linewidth=2.0, label=\"Test losses\")\n",
        "plt.plot(indep_test_axis, np.array(test_accuracies), \"b-\", linewidth=2.0, label=\"Test accuracies\")\n",
        "print(len(test_accuracies))\n",
        "print(len(train_accuracies))\n",
        "\n",
        "plt.title(\"Training performance over Iterations\")\n",
        "plt.legend(loc='lower right', shadow=True)\n",
        "plt.ylabel('Training Performance')\n",
        "plt.xlabel('Training Iteration')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "predictions = one_hot_predictions.argmax(1)\n",
        "\n",
        "print(\"Testing Accuracy: {}%\".format(100*accuracy))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Precision: {}%\".format(100*metrics.precision_score(y_test, predictions, average=\"weighted\")))\n",
        "print(\"Recall: {}%\".format(100*metrics.recall_score(y_test, predictions, average=\"weighted\")))\n",
        "print(\"f1_score: {}%\".format(100*metrics.f1_score(y_test, predictions, average=\"weighted\")))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(\"Created using test set of {} datapoints, normalised to % of each class in the test dataset\".format(len(y_test)))\n",
        "confusion_matrix = metrics.confusion_matrix(y_test, predictions)\n",
        "\n",
        "\n",
        "#print(confusion_matrix)\n",
        "normalised_confusion_matrix = np.array(confusion_matrix, dtype=np.float32)/np.sum(confusion_matrix)*100\n",
        "\n",
        "\n",
        "width = 12\n",
        "height = 12\n",
        "plt.figure(figsize=(width, height))\n",
        "plt.imshow(\n",
        "    normalised_confusion_matrix, \n",
        "    interpolation='nearest', \n",
        "    cmap=plt.cm.Blues\n",
        ")\n",
        "plt.title(\"Confusion matrix \\n(normalised to % of total test data)\")\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(n_classes)\n",
        "plt.xticks(tick_marks, LABELS, rotation=90)\n",
        "plt.yticks(tick_marks, LABELS)\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1eX4EBB7HIED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Models"
      ],
      "metadata": {
        "id": "oeb3eTyTQti6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, f1_score, precision_score, recall_score\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics\n",
        "import warnings\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "# tune regularization for multinomial logistic regression\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from matplotlib import pyplot"
      ],
      "metadata": {
        "id": "_0XVRUYaQ0eU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "tEFLT4f1Q15x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_df = pd.read_csv('/content/drive/MyDrive/Data_file_for_cis520_project/Feature Extraction (Normalized)/01/0.csv',sep=',')\n",
        "print(feature_df)\n",
        "feature_df.info()"
      ],
      "metadata": {
        "id": "TlNyzVVpQ3cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_df = pd.read_csv('/content/drive/MyDrive/Data_file_for_cis520_project/Feature Extraction/01/0_labels.csv',sep=',')\n",
        "print(label_df)\n",
        "label_df.info()"
      ],
      "metadata": {
        "id": "_QUZgzg6Q3lC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "my_list = os.listdir('/content/drive/MyDrive/Data_file_for_cis520_project/Feature Extraction (Normalized)')"
      ],
      "metadata": {
        "id": "qW5oBzW2Q7x5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = pd.read_csv('/content/drive/MyDrive/Data_file_for_cis520_project/Feature Extraction (Normalized)/01/0.csv', header=None).values\n",
        "X = np.vstack([X, pd.read_csv('/content/drive/MyDrive/Data_file_for_cis520_project/Feature Extraction (Normalized)/01/5.csv', header=None).values])\n",
        "X = np.vstack([X, pd.read_csv('/content/drive/MyDrive/Data_file_for_cis520_project/Feature Extraction (Normalized)/01/10.csv', header=None).values])\n",
        "Y = pd.read_csv('/content/drive/MyDrive/Data_file_for_cis520_project/Feature Extraction/01/0_labels.csv', header=None).values\n",
        "Y = np.vstack([Y, pd.read_csv('/content/drive/MyDrive/Data_file_for_cis520_project/Feature Extraction/01/5_labels.csv', header=None).values])\n",
        "Y = np.vstack([Y, pd.read_csv('/content/drive/MyDrive/Data_file_for_cis520_project/Feature Extraction/01/10_labels.csv', header=None).values])\n",
        "for i in my_list:\n",
        "    for j in [0, 5, 10]:\n",
        "        # print('/content/drive/MyDrive/Data_file_for_cis520_project/Feature Extraction/' + s + '/' + str(j) + '_features.csv')\n",
        "        # print('/content/drive/MyDrive/Data_file_for_cis520_project/Feature Extraction/' + s + '/' + str(j) + '_labels.csv')\n",
        "        X = np.vstack([X, pd.read_csv('/content/drive/MyDrive/Data_file_for_cis520_project/Feature Extraction (Normalized)/' + i + '/' + str(j) + '.csv', header=None).values])\n",
        "        Y = np.vstack([Y, pd.read_csv('/content/drive/MyDrive/Data_file_for_cis520_project/Feature Extraction/' + i + '/' + str(j) + '_labels.csv', header=None).values])\n",
        "\n",
        "Y = Y.astype(int)\n",
        "\n",
        "np.savetxt(\"/content/drive/MyDrive/Data_file_for_cis520_project/FeatureCombined/X.csv\", X, delimiter=\",\")\n",
        "np.savetxt(\"/content/drive/MyDrive/Data_file_for_cis520_project/FeatureCombined/Y.csv\", Y, delimiter=\",\")"
      ],
      "metadata": {
        "id": "yHn02IjFQ70O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression"
      ],
      "metadata": {
        "id": "fK_EAQaKQwQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_models():\n",
        "\tmodels = dict()\n",
        "\tfor p in [0.0, 0.0001, 0.001, 0.01, 0.1, 1.0]:\n",
        "\t\t# create name for model\n",
        "\t\tkey = '%.4f' % p\n",
        "\t\t# turn off penalty in some cases\n",
        "\t\tif p == 0.0:\n",
        "\t\t\t# no penalty in this case\n",
        "\t\t\tmodels[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='none')\n",
        "\t\telse:\n",
        "\t\t\tmodels[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', C=p)\n",
        "\treturn models"
      ],
      "metadata": {
        "id": "uPBmk9BmHIGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X, y):\n",
        "\t# define the evaluation procedure\n",
        "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\t# evaluate the model\n",
        "\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "\treturn scores"
      ],
      "metadata": {
        "id": "CLyGxSJ_Q-6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size = len(Y)\n",
        "# p = np.random.permutation(size)\n",
        "# X = X[p]\n",
        "# Y = Y[p]\n",
        "\n",
        "train_size = int(size * 0.8)\n",
        "X_train = X[:train_size,:]\n",
        "y_train = Y[:train_size,:]\n",
        "X_test = X[train_size+1:,:]\n",
        "y_test = Y[train_size+1:,:]"
      ],
      "metadata": {
        "id": "n7tndYADQ-9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = get_models()\n",
        "# evaluate the models and store results\n",
        "results, names = list(), list()\n",
        "for name, model in models.items():\n",
        "\t# evaluate the model and collect the scores\n",
        "\tscores = evaluate_model(model, X_train, y_train)\n",
        "\t# store the results\n",
        "\tresults.append(scores)\n",
        "\tnames.append(name)\n",
        "\t# summarize progress along the way\n",
        "\tprint('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
        "# plot model performance for comparison\n",
        "pyplot.boxplot(results, labels=names, showmeans=True)\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "eefZCBkWQ-_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(multi_class='multinomial', penalty='none')"
      ],
      "metadata": {
        "id": "Fpe7H-YlQ_Bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import model_selection\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "results = cross_val_score(model, X, Y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "print(\"Accuracy: %.3f (%.3f)\" % (results.mean(), results.std()))"
      ],
      "metadata": {
        "id": "p36jBp_0Q_EW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import make_scorer\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "f1_scorer = make_scorer(f1_score, pos_label=\"yes\", average='macro')\n",
        "results = cross_val_score(model, pd.DataFrame(X), pd.DataFrame(Y), scoring=f1_scorer, cv=cv, n_jobs=-1, error_score=\"raise\")\n",
        "print(\"f1: %.3f (%.3f)\" % (results.mean(), results.std()))"
      ],
      "metadata": {
        "id": "ZWBUs4IeRNGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "precision_score = make_scorer(precision_score, pos_label=\"yes\", average='macro')\n",
        "results = cross_val_score(model, X, Y, scoring=precision_score, cv=cv, n_jobs=-1)\n",
        "print(\"precision: %.3f (%.3f)\" % (results.mean(), results.std()))"
      ],
      "metadata": {
        "id": "fHT5HNR6RNI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "recall_score = make_scorer(recall_score, pos_label=\"yes\", average='macro')\n",
        "results = cross_val_score(model, X, Y, scoring=recall_score, cv=cv, n_jobs=-1)\n",
        "print(\"recall: %.3f (%.3f)\" % (results.mean(), results.std()))"
      ],
      "metadata": {
        "id": "xF2tJll6RNLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "roc_auc_score = make_scorer(roc_auc_score, pos_label=\"yes\", average='macro')\n",
        "results = cross_val_score(model, X, Y, scoring=roc_auc_score, cv=cv, n_jobs=-1)\n",
        "print(\"roc_auc: %.3f (%.3f)\" % (results.mean(), results.std()))"
      ],
      "metadata": {
        "id": "XjSqpU9iRNNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dummy Classification"
      ],
      "metadata": {
        "id": "jv6Qlkc5RSx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "dummy_model = DummyClassifier(strategy=\"uniform\")\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "results = cross_val_score(dummy_model, X, Y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "print(\"Accuracy: %.3f (%.3f)\" % (results.mean(), results.std()))"
      ],
      "metadata": {
        "id": "AM50DevYRNQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "dummy_model = DummyClassifier(strategy=\"most_frequent\")\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "results = cross_val_score(dummy_model, X, Y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "print(\"Accuracy: %.3f (%.3f)\" % (results.mean(), results.std()))"
      ],
      "metadata": {
        "id": "T5O9gI8FRNSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Naive Bayes"
      ],
      "metadata": {
        "id": "drHUmX7fRYDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train);"
      ],
      "metadata": {
        "id": "t00H8OaQRXf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "ouDdVPoDRXin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=50, cmap='RdBu')\n",
        "lim = plt.axis()\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred, s=20, cmap='RdBu', alpha=0.1)\n",
        "plt.axis(lim);"
      ],
      "metadata": {
        "id": "EG9dwa2yRXk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mpl_toolkits import mplot3d\n",
        "\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "idx = np.squeeze(y_train) == 0\n",
        "\n",
        "arr = np.squeeze(y_train)\n",
        "arr[np.squeeze(y_train) == 0] = 1\n",
        "arr[np.squeeze(y_train) == 5] = 5\n",
        "arr[np.squeeze(y_train) == 10] = 10\n",
        "\n",
        "\n",
        "ax = plt.axes(projection='3d')\n",
        "\n",
        "# Data for three-dimensional scattered points\n",
        "zdata = X_train[:, 0]\n",
        "xdata = X_train[:, 1]\n",
        "ydata = X_train[:, 2]\n",
        "output = X_train\n",
        "ax.scatter3D(ydata, xdata, zdata, s=arr, c=X_train[:, 3], cmap='nipy_spectral');"
      ],
      "metadata": {
        "id": "CqYhin3nRXm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mpl_toolkits import mplot3d\n",
        "\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "idx = np.squeeze(y_train) == 0\n",
        "\n",
        "ax = plt.axes(projection='3d')\n",
        "\n",
        "# Data for three-dimensional scattered points\n",
        "zdata = X_train[idx, 0]\n",
        "xdata = X_train[idx, 1]\n",
        "ydata = X_train[idx, 2]\n",
        "output = X_train\n",
        "ax.scatter3D(ydata, xdata, zdata, c=X_train[idx, 3], cmap='nipy_spectral');"
      ],
      "metadata": {
        "id": "CmbWTKCERXpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mpl_toolkits import mplot3d\n",
        "\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "idx = np.squeeze(y_train) == 5\n",
        "\n",
        "ax = plt.axes(projection='3d')\n",
        "\n",
        "# Data for three-dimensional scattered points\n",
        "zdata = X_train[idx, 0]\n",
        "xdata = X_train[idx, 1]\n",
        "ydata = X_train[idx, 2]\n",
        "output = X_train\n",
        "ax.scatter3D(ydata, xdata, zdata, c=X_train[idx, 3], cmap='nipy_spectral');"
      ],
      "metadata": {
        "id": "5kjT4LqIRXrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mpl_toolkits import mplot3d\n",
        "\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "idx = np.squeeze(y_train) == 10\n",
        "\n",
        "ax = plt.axes(projection='3d')\n",
        "\n",
        "# Data for three-dimensional scattered points\n",
        "zdata = X_train[idx, 0]\n",
        "xdata = X_train[idx, 1]\n",
        "ydata = X_train[idx, 2]\n",
        "output = X_train\n",
        "ax.scatter3D(ydata, xdata, zdata, c=X_train[idx, 3], cmap='nipy_spectral');"
      ],
      "metadata": {
        "id": "MoZUMb9bRNUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import zero_one_loss\n",
        "1 - zero_one_loss(np.squeeze(y_test), y_pred)"
      ],
      "metadata": {
        "id": "FURbbhpYRNXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KNN"
      ],
      "metadata": {
        "id": "nUi7Ppx5R9tK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn import datasets\n",
        "from skimage import exposure\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "YrwLzlc2R8Yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracies = []\n",
        "\n",
        "# loop over various values of `k` for the k-Nearest Neighbor classifier\n",
        "\n",
        "for k in range(1, 50, 2):\n",
        "  model = KNeighborsClassifier(n_neighbors=k)\n",
        "  model.fit(X_train, y_train)\n",
        "  score = model.score(X_test, y_test)\n",
        "  accuracies.append(score)"
      ],
      "metadata": {
        "id": "ZMMQwKWQR_pJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k_values = range(1, 50, 2)\n",
        "plt.plot(k_values, accuracies, color='g')\n",
        "plt.xlabel(\"K values\")\n",
        "plt.ylabel(\"Validation Accuracy\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_GujF8NER_rV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Tree"
      ],
      "metadata": {
        "id": "iq7u7t0ySDPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "metadata": {
        "id": "I5lhONuMR_tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "depth= [2,4,8,16,32, 64, 128]\n",
        "random = [10,20,40,80,160]\n",
        "\n",
        "for d in depth:\n",
        "    for r in random:\n",
        "        tree_clf = DecisionTreeClassifier(max_depth=d, random_state=r)\n",
        "        tree_clf.fit(X_train, y_train)\n",
        "        print(\"D: \" + str(d) + \" , R: \" + str(r) + \" , A: \" + str(tree_clf.score(X_test, y_test)))"
      ],
      "metadata": {
        "id": "Kpox_ggOR_w5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest"
      ],
      "metadata": {
        "id": "4UJhJWA2SJ6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "depth= [2,4,8,16,32, 64, 128]\n",
        "random = [0, 10,20,40,80,160]\n",
        "\n",
        "for d in depth:\n",
        "    for r in random:\n",
        "        clf = RandomForestClassifier(max_depth=d, random_state=r)\n",
        "        clf.fit(X_train, y_train)\n",
        "        print(\"D: \" + str(d) + \" , R: \" + str(r) + \" , A: \" + str(clf.score(X_test, y_test)))"
      ],
      "metadata": {
        "id": "blgBSCEgR_ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "X_bi_train = X_train[np.squeeze(np.logical_or(y_train==0, y_train == 10)),:]\n",
        "y_bi_train = y_train[np.squeeze(np.logical_or(y_train==0, y_train == 10))]\n",
        "X_bi_test = X_test[np.squeeze(np.logical_or(y_test==0, y_test == 10)),:]\n",
        "y_bi_test = y_test[np.squeeze(np.logical_or(y_test==0, y_test == 10))]\n",
        "\n",
        "depth= [2,4,8,16,32, 64, 128]\n",
        "random = [0, 10,20,40,80,160]\n",
        "\n",
        "for d in depth:\n",
        "    for r in random:\n",
        "        clf = RandomForestClassifier(max_depth=d, random_state=r)\n",
        "        clf.fit(X_bi_train, y_bi_train)\n",
        "        print(\"D: \" + str(d) + \" , R: \" + str(r) + \" , A: \" + str(clf.score(X_bi_test, y_bi_test)))"
      ],
      "metadata": {
        "id": "0XDsD_f6R_0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AutoML Model with Major Vote"
      ],
      "metadata": {
        "id": "n_Xou0dDSTn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install auto-sklearn"
      ],
      "metadata": {
        "id": "S4o-T2PHR_22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import operator\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import autosklearn.classification\n",
        "\n",
        "loaded_X = []\n",
        "loaded_y = []\n",
        "\n",
        "for j in range(1, 61): \n",
        "    if (j < 10):\n",
        "        s = \"0\"+str(j)\n",
        "    else:\n",
        "        s = str(j)\n",
        "    for i in [0, 5, 10]:\n",
        "        loaded_X.append(np.loadtxt(\"/content/drive/MyDrive/Data_file_for_cis520_project/All_Landmarks/\"+s+\"/\"+str(i)+\"_features.csv\", delimiter=','))\n",
        "        loaded_y.append(np.loadtxt(\"/content/drive/MyDrive/Data_file_for_cis520_project/All_Landmarks/\"+s+\"/\"+str(i)+\"_labels.csv\", delimiter=','))\n",
        "\n",
        "loaded_new_X=[]\n",
        "loaded_new_y=[]\n",
        "for i in range(len(loaded_X)):\n",
        "  for j in range(len(loaded_X[i])):\n",
        "    loaded_new_X.append(loaded_X[i][j])\n",
        "    loaded_new_y.append(loaded_y[i][j])\n",
        "\n",
        "#Splitting the data\n",
        "train_X=np.array(loaded_new_X)[:70000,:]\n",
        "train_y=np.array(loaded_new_y)[:70000]\n",
        "test_X=np.array(loaded_new_X)[70000:,:]\n",
        "test_y=np.array(loaded_new_y)[70000:]\n",
        "\n",
        "\n",
        "shuffler1 = np.random.permutation(len(train_X))\n",
        "shuffler2 = np.random.permutation(len(test_X))\n",
        "train_X = train_X[shuffler1]\n",
        "train_y = train_y[shuffler1]\n",
        "test_X = test_X[shuffler2]\n",
        "test_y = test_y[shuffler2]\n",
        "\n",
        "import sklearn.metrics\n",
        "import autosklearn.classification\n",
        "\n",
        "automl = autosklearn.classification.AutoSklearnClassifier(\n",
        "    time_left_for_this_task=600,\n",
        "    per_run_time_limit=120, \n",
        "    resampling_strategy=\"cv\",\n",
        "    resampling_strategy_arguments={'folds': 4},\n",
        "    )\n",
        "\n",
        "automl.fit(train_X,\n",
        "          train_y, test_X, test_y, dataset_name=\"Drowsiness_detection_automl_model\")\n",
        "\n",
        "import sklearn.metrics\n",
        "predictions = automl.predict(test_X)\n",
        "print(\"Accuracy score\", sklearn.metrics.accuracy_score(test_y, predictions))"
      ],
      "metadata": {
        "id": "l71qXOBsSaww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_X = []\n",
        "batch_y = []\n",
        "\n",
        "for j in range(50, 61): \n",
        "    if (j < 10):\n",
        "        s = \"0\"+str(j)\n",
        "    else:\n",
        "        s = str(j)\n",
        "    for i in [0, 5, 10]:\n",
        "        loaded_X = np.loadtxt(\"/content/drive/MyDrive/Data_file_for_cis520_project/All_Landmarks/\"+s+\"/\"+str(i)+\"_features.csv\", delimiter=',')\n",
        "        length = (int) (loaded_X.shape[0] / 3)\n",
        "        offset = (int) (length / 5)\n",
        "        for k in range(5,length + 1,5):\n",
        "            idx = np.arange(k, loaded_X.shape[0], length)\n",
        "            time_series = np.split(loaded_X, idx)\n",
        "            if len(time_series)>=2:\n",
        "                del time_series[-1]\n",
        "                batch_X += time_series\n",
        "                y_series = [int(i/5)] * len(time_series)\n",
        "                batch_y += y_series\n",
        "\n",
        "batch_y = (np.array(batch_y) * 5).tolist()\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "def pipline(model, train_data):\n",
        "    result = automl.predict(train_data)\n",
        "    data = Counter(result)\n",
        "    # print(result)\n",
        "    return data.most_common(1)[0][0]\n",
        "\n",
        "prediction = []\n",
        "for bx in batch_X:\n",
        "    prediction.append(pipline(automl, bx))\n",
        "\n",
        "    \n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics\n",
        "\n",
        "accuracy_score(batch_y, prediction)\n",
        "\n",
        "n_classes = 3\n",
        "LABELS = [\n",
        "    'alert',\n",
        "    'low vigilant',\n",
        "    'drowsy',\n",
        "]\n",
        "\n",
        "confusion_matrix = metrics.confusion_matrix(batch_y, prediction)\n",
        "\n",
        "\n",
        "#print(confusion_matrix)\n",
        "normalised_confusion_matrix = np.array(confusion_matrix, dtype=np.float32)/np.sum(confusion_matrix)*100\n",
        "\n",
        "\n",
        "#print(confusion_matrix)\n",
        "normalised_confusion_matrix = np.array(confusion_matrix, dtype=np.float32)/np.sum(confusion_matrix)*100\n",
        "\n",
        "\n",
        "width = 12\n",
        "height = 12\n",
        "plt.figure(figsize=(width, height))\n",
        "plt.imshow(\n",
        "    normalised_confusion_matrix, \n",
        "    interpolation='nearest', \n",
        "    cmap=plt.cm.Blues\n",
        ")\n",
        "plt.title(\"Confusion matrix \\n(normalised to % of total test data)\")\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(n_classes)\n",
        "plt.xticks(tick_marks, LABELS, rotation=90)\n",
        "plt.yticks(tick_marks, LABELS)\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ChQDGIiHSazA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NVn02UfJSy3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AUTO ML"
      ],
      "metadata": {
        "id": "NFHnHAHEpD0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install auto-sklearn"
      ],
      "metadata": {
        "id": "sQFHiZxOSa1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train accuracy\n",
        "import sklearn.metrics\n",
        "predictions_train = automl.predict(train_X)\n",
        "print(\"Accuracy score for training set\", sklearn.metrics.accuracy_score(train_y, predictions))\n",
        "predictions_test = automl.predict(train_X)\n",
        "print(\"Accuracy score for testing set\", sklearn.metrics.accuracy_score(train_y, predictions))"
      ],
      "metadata": {
        "id": "k8aD4msaSa56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8Gd21oaqSa8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mIeXSOtgSbAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fRcJDnfNSbCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "g_I_POrASbDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JlbGB1zYSbFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ALpxjwsMSbHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9FyDd0J2R_5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jl6tw3aYR_7a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}